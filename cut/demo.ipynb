{"cells":[{"cell_type":"markdown","metadata":{"id":"0MlT2nnmaJqH"},"source":["# Demo of NCut algorithm\n","\n","This notebook provides a demo of NCut algorithm that I use in my thesis project. You can upload your image to see how well NCut algorithm work to identify the salient object and produce its mask."]},{"cell_type":"markdown","metadata":{"id":"ekzsPrM4m9pG"},"source":["## Install and import necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77847,"status":"ok","timestamp":1731772316682,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"Qzf_3jx8uiJX","outputId":"ff99f217-782a-435a-900f-ff59acdd8d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting toponetx\n","  Downloading TopoNetX-0.1.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from toponetx) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from toponetx) (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from toponetx) (2.2.2)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from toponetx) (17.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from toponetx) (2.32.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from toponetx) (1.13.1)\n","Collecting trimesh (from toponetx)\n","  Downloading trimesh-4.5.2-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from toponetx) (4.12.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->toponetx) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->toponetx) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->toponetx) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->toponetx) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->toponetx) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->toponetx) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->toponetx) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->toponetx) (1.16.0)\n","Downloading TopoNetX-0.1.1-py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trimesh-4.5.2-py3-none-any.whl (704 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.4/704.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh, toponetx\n","Successfully installed toponetx-0.1.1 trimesh-4.5.2\n","Collecting pymatting\n","  Downloading PyMatting-1.1.13-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymatting) (1.26.4)\n","Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from pymatting) (11.0.0)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting) (0.60.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pymatting) (1.13.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting) (0.43.0)\n","Downloading PyMatting-1.1.13-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymatting\n","Successfully installed pymatting-1.1.13\n","Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n","  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-k9c7r2d4\n","  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-k9c7r2d4\n","  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pydensecrf\n","  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pydensecrf: filename=pydensecrf-1.0-cp310-cp310-linux_x86_64.whl size=3405189 sha256=bfd464c9fcbbddac12fac1390614b29c294cd0edec9e6f3e257e90df3aa5cc9c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mw0c4cs7/wheels/01/5b/61/87443ed3bf03dd2940375cf2f8b6fba88efece935465e490b0\n","Successfully built pydensecrf\n","Installing collected packages: pydensecrf\n","Successfully installed pydensecrf-1.0\n"]}],"source":["%pip install toponetx\n","%pip install pymatting\n","%pip install git+https://github.com/lucasb-eyer/pydensecrf.git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":23166,"status":"ok","timestamp":1731772339845,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"nqpxKPs7ptIq"},"outputs":[],"source":["import torch\n","import numpy as np\n","from scipy.sparse.linalg import eigsh\n","import matplotlib.pyplot as plt\n","import PIL.Image as Image\n","from scipy.sparse import csr_matrix, diags, coo_matrix\n","import toponetx as tnx\n","import torch.nn as nn\n","import PIL\n","from torchvision import transforms\n","from google.colab import files\n","from scipy.sparse.linalg import eigsh\n","from scipy import ndimage\n","import torch.nn.functional as F\n","import pydensecrf.densecrf as dcrf\n","import pydensecrf.utils as utils"]},{"cell_type":"markdown","metadata":{"id":"wql0z4ZEnFzA"},"source":["## Necessary blocks\n","These code block should be executed before running the NCut algorithm.\n","- ViTFeat - to be utilized as feature extractor\n","- densecrf - CRF implementation to be utilized as post-processor"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731772339845,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"KoIrB3b5nUu8"},"outputs":[],"source":["class ViTFeat(nn.Module):\n","  \"\"\" Vision Transformer \"\"\"\n","  def __init__(self, vit_arch='base', vit_feat='k', patch_size=16, vit_model='dino', pretrained_dir=None):\n","    super().__init__()\n","    self.vit_feat = vit_feat\n","    self.patch_size = patch_size\n","    self.vit_model = vit_model\n","    dino_dir = \"facebookresearch/dino:main\"\n","    dinov2_dir = \"facebookresearch/dinov2\"\n","\n","    if self.vit_model == 'dino':\n","      if vit_arch == 'small':\n","        if self.patch_size == 16:\n","          model = torch.hub.load(dino_dir, \"dino_vits16\")\n","        elif self.patch_size == 8:\n","          model = torch.hub.load(dino_dir, \"dino_vits8\")\n","        self.feat_dim = 384\n","      elif vit_arch == 'base':\n","        if self.patch_size == 16:\n","          model = torch.hub.load(dino_dir, \"dino_vitb16\")\n","        elif self.patch_size == 8:\n","          model = torch.hub.load(dino_dir, \"dino_vitb8\")\n","        self.feat_dim = 768\n","\n","    model.eval()\n","    self.model = model\n","\n","  def forward(self, img):\n","    if self.vit_model in ['sammed', 'sam', 'sam2']:\n","      embed = self.model(img)\n","      bs, feat_dim, h, w = embed.shape[0], embed.shape[1], embed.shape[2], embed.shape[3]\n","      embed = embed.reshape(bs, feat_dim, h*w)\n","      return embed\n","\n","    hook_out = {}\n","    def hook_fn_forward_qkv(module, input, output):\n","      hook_out[\"qkv\"] = output\n","    self.model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n","\n","    def hook_fn_last_attention(module, input, output):\n","      hook_out[\"attn\"] = output\n","\n","    if self.vit_model == 'dinov2':\n","      self.model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"attn_drop\"].register_forward_hook(hook_fn_last_attention)\n","    elif self.vit_model == 'dino':\n","      self.model._modules[\"blocks\"][-1]._modules[\"attn\"].register_forward_hook(hook_fn_last_attention)\n","\n","    # Forward pass in the model\n","    with torch.no_grad() :\n","      h, w = img.shape[2], img.shape[3]\n","      feat_h, feat_w = h // self.patch_size, w // self.patch_size\n","      if self.vit_model == 'dinov2':\n","        #outputs = self.model.get_intermediate_layers(img, 12)\n","        outputs = self.model(img)\n","        bs, nb_head, nb_token = hook_out[\"attn\"].shape[0], hook_out[\"attn\"].shape[1], hook_out[\"attn\"].shape[2]\n","      else:\n","        attentions = self.model.get_last_selfattention(img)\n","        bs, nb_head, nb_token = attentions.shape[0], attentions.shape[1], attentions.shape[2]\n","\n","      qkv = (\n","              hook_out[\"qkv\"]\n","              .reshape(bs, nb_token, 3, nb_head, -1)\n","              .permute(2, 0, 3, 1, 4)\n","          )\n","      q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","      k = k.transpose(1, 2).reshape(bs, nb_token, -1)\n","      q = q.transpose(1, 2).reshape(bs, nb_token, -1)\n","      v = v.transpose(1, 2).reshape(bs, nb_token, -1)\n","\n","      # Modality selection\n","      if self.vit_feat == \"k\":\n","        feats = k[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","      elif self.vit_feat == \"q\":\n","        feats = q[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","      elif self.vit_feat == \"v\":\n","        feats = v[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","      elif self.vit_feat == \"kqv\":\n","        k = k[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","        q = q[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","        v = v[:, 1:].transpose(1, 2).reshape(bs, self.feat_dim, feat_h * feat_w)\n","        feats = torch.cat([k, q, v], dim=1)\n","      return feats"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731772339845,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"OyfM4y0re7aL"},"outputs":[],"source":["MAX_ITER = 10\n","POS_W = 7\n","POS_XY_STD = 3\n","Bi_W = 10\n","Bi_XY_STD = 50\n","Bi_RGB_STD = 5\n","\n","def densecrf(image, mask):\n","    h, w = mask.shape\n","    mask = mask.reshape(1, h, w)\n","    fg = mask.astype(float)\n","    bg = 1 - fg\n","    output_logits = torch.from_numpy(np.concatenate((bg,fg), axis=0))\n","\n","    H, W = image.shape[:2]\n","    image = np.ascontiguousarray(image)\n","\n","    output_logits = F.interpolate(output_logits.unsqueeze(0), size=(H, W), mode=\"bilinear\").squeeze()\n","    output_probs = F.softmax(output_logits, dim=0).cpu().numpy()\n","\n","    c = output_probs.shape[0]\n","    h = output_probs.shape[1]\n","    w = output_probs.shape[2]\n","\n","    U = utils.unary_from_softmax(output_probs)\n","    U = np.ascontiguousarray(U)\n","\n","    d = dcrf.DenseCRF2D(w, h, c)\n","    d.setUnaryEnergy(U)\n","    d.addPairwiseGaussian(sxy=POS_XY_STD, compat=POS_W)\n","    d.addPairwiseBilateral(sxy=Bi_XY_STD, srgb=Bi_RGB_STD, rgbim=image, compat=Bi_W)\n","\n","    Q = d.inference(MAX_ITER)\n","    Q = np.array(Q).reshape((c, h, w))\n","    MAP = np.argmax(Q, axis=0).reshape((h,w)).astype(np.float32)\n","    return MAP"]},{"cell_type":"markdown","metadata":{"id":"g9-jxJVcnU9v"},"source":["## NCut functions\n","- To compute;\n","  - the combined affinity matrix\n","  - the laplacian matrix\n","  - the second smallest eigenvector of laplacian matrix\n","- To detect\n","  - salient object in image"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731772339845,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"ZLsbKRrFpUZc"},"outputs":[],"source":["def knn_affinity(image, n_neighbors=[20, 10], distance_weights=[2.0, 0.1]):\n","    \"\"\"Computes a KNN-based affinity matrix. Note that this function requires pymatting\"\"\"\n","    try:\n","        from pymatting.util.kdtree import knn\n","    except:\n","        raise ImportError(\n","            'Please install pymatting to compute KNN affinity matrices:\\n'\n","            'pip3 install pymatting'\n","        )\n","\n","    h, w = image.shape[:2]\n","    r, g, b = image.reshape(-1, 3).T\n","    n = w * h\n","\n","    x = np.tile(np.linspace(0, 1, w), h)\n","    y = np.repeat(np.linspace(0, 1, h), w)\n","\n","    i, j = [], []\n","    for k, distance_weight in zip(n_neighbors, distance_weights):\n","        f = np.stack(\n","            [r, g, b, distance_weight * x, distance_weight * y],\n","            axis=1,\n","            out=np.zeros((n, 5), dtype=np.float32),\n","        )\n","\n","        distances, neighbors = knn(f, f, k=k)\n","\n","        i.append(np.repeat(np.arange(n), k))\n","        j.append(neighbors.flatten())\n","\n","    ij = np.concatenate(i + j)\n","    ji = np.concatenate(j + i)\n","    coo_data = np.ones(2 * sum(n_neighbors) * n)\n","\n","    # This is our affinity matrix\n","    W = csr_matrix((coo_data, (ij, ji)), (n, n))\n","    return W\n","\n","def get_diagonal(W, threshold: float = 1e-12):\n","  \"\"\"Gets the diagonal sum of a sparse matrix\"\"\"\n","  try:\n","      from pymatting.util.util import row_sum\n","  except:\n","      raise ImportError(\n","          'Please install pymatting to compute the diagonal sums:\\n'\n","          'pip3 install pymatting'\n","      )\n","\n","  D = row_sum(W)\n","  D[D < threshold] = 1.0  # Prevent division by zero.\n","  D = diags(D)\n","  return D\n","\n","def compute_eigs(L, D, K: int = 2):\n","  # Extract eigenvectors\n","  try:\n","      eigenvalues, eigenvectors = eigsh(L, sigma=0, k=K, which='LM', M=D)\n","  except:\n","      eigenvalues, eigenvectors = eigsh(L, k=K, which='SM', M=D)\n","\n","  eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n","\n","  # Sign ambiguity\n","  for k in range(eigenvectors.shape[0]):\n","      if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n","          eigenvectors[k] = 0 - eigenvectors[k]\n","\n","  return eigenvectors\n","\n","def get_salient_areas(second_smallest_vec):\n","  # get the area corresponding to salient objects.\n","  avg = np.sum(second_smallest_vec) / len(second_smallest_vec)\n","  bipartition = second_smallest_vec > avg  #0\n","  return bipartition\n","\n","def check_num_fg_corners(bipartition, dims):\n","  # check number of corners belonging to the foreground\n","  bipartition_ = bipartition.reshape(dims)\n","  top_l, top_r, bottom_l, bottom_r = bipartition_[0][0], bipartition_[0][-1], bipartition_[-1][0], bipartition_[-1][-1]\n","  nc = int(top_l) + int(top_r) + int(bottom_l) + int(bottom_r)\n","  return nc\n","\n","def compute_laplacian(W, lap=\"comb\", s=1.0):\n","  D = np.array(get_diagonal(W).todense())\n","  if lap == \"comb\":\n","      # combinatorial laplacian\n","      L = D - W\n","  elif lap == \"norm\":\n","      # normalized laplacian\n","      D_inv_sqrt = np.linalg.pinv(np.sqrt(D))\n","      I = np.eye(W.shape[0])\n","      L = s*I - D_inv_sqrt @ W @ D_inv_sqrt\n","  elif lap == \"rw\":\n","      # random walk laplacian\n","      D_inv = np.linalg.pinv(D)\n","      I = np.eye(W.shape[0])\n","      L = s*I - D_inv @ W\n","  else:\n","      raise NotImplementedError()\n","\n","  return L.astype(np.float32), D.astype(np.float32)\n","\n","def detect_box(bipartition, seed,  dims, initial_im_size=None, scales=None, principle_object=True):\n","  \"\"\"\n","  Extract a box corresponding to the seed patch. Among connected components extract from the affinity matrix, select the one corresponding to the seed patch.\n","  \"\"\"\n","  w_featmap, h_featmap = dims\n","  objects, num_objects = ndimage.label(bipartition)\n","  cc = objects[np.unravel_index(seed, dims)]\n","\n","  if principle_object:\n","      mask = np.where(objects == cc)\n","      # Add +1 because excluded max\n","      ymin, ymax = min(mask[0]), max(mask[0]) + 1\n","      xmin, xmax = min(mask[1]), max(mask[1]) + 1\n","      # Rescale to image size\n","      r_xmin, r_xmax = scales[1] * xmin, scales[1] * xmax\n","      r_ymin, r_ymax = scales[0] * ymin, scales[0] * ymax\n","      pred = [r_xmin, r_ymin, r_xmax, r_ymax]\n","\n","      # Check not out of image size (used when padding)\n","      if initial_im_size:\n","          pred[2] = min(pred[2], initial_im_size[1])\n","          pred[3] = min(pred[3], initial_im_size[0])\n","\n","      # Coordinate predictions for the feature space\n","      # Axis different then in image space\n","      pred_feats = [ymin, xmin, ymax, xmax]\n","\n","      return pred, pred_feats, objects, mask\n","  else:\n","      raise NotImplementedError\n","\n","# Image transformation applied to all images\n","ToTensor = transforms.Compose([transforms.ToTensor(),\n","                               transforms.Normalize(\n","                                (0.485, 0.456, 0.406),\n","                                (0.229, 0.224, 0.225)),])"]},{"cell_type":"markdown","metadata":{"id":"k01Xi8UOnwst"},"source":["## Run NCut algorithm\n","First upload any desired image to be partitioned.\n","Then simply run the second code block to extract the bipartitioned area in image as mask.\n","\n","You can change the parameters of the experiment:\n","- patch_size: The patch size of feature extractor backbone; 8 or 16\n","- fixed_size: The fixed size to read the input image.\n","- laplacian: Which laplacian matrix to compute; comb, norm, rw or hodge\n","- vit_arch: Backbone architecture; small, base, large\n","- vit_feat: Vector channel to extract features from. Best features are extracted from 'k'; q, k or v"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":13597,"status":"ok","timestamp":1731773079638,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"bA6nCAfiamvS","outputId":"ceaabf08-d6fe-4fb5-cb2d-61aee9cd72f2"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-fabdea11-d2bf-4ab4-81e8-219cc4e592da\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-fabdea11-d2bf-4ab4-81e8-219cc4e592da\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving les02.JPG to les02.JPG\n"]}],"source":["uploaded = files.upload()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2436,"status":"ok","timestamp":1731773244614,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"mNnnOeEJnotl","outputId":"71b963ce-bc94-4ef6-c537-d88c6ab74285"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"]}],"source":["patch_size = 16\n","fixed_size = 480\n","laplacian = 'comb'\n","vit_arch = 'small'\n","vit_feat = 'k'\n","\n","backbone = ViTFeat(vit_arch=vit_arch, vit_feat=vit_feat, patch_size=patch_size)\n","backbone.eval()\n","if torch.cuda.is_available(): backbone.cuda()\n","\n","# open image\n","I = Image.open(list(uploaded.keys())[0]).convert('RGB')\n","image = I.resize((int(fixed_size), int(fixed_size)), PIL.Image.LANCZOS)\n","\n","# extract feature from backbone\n","tensor = ToTensor(image).unsqueeze(0)\n","if torch.cuda.is_available(): tensor = tensor.cuda()\n","feats = backbone(tensor)[0]\n","\n","# prepare image and features\n","H_patch, W_patch = image.size[0] // patch_size, image.size[1] // patch_size\n","image_rgb = image.resize((H_patch, W_patch), Image.BILINEAR)\n","image = image_rgb.convert('HSV')\n","image = np.array(image)\n","feats = torch.t(feats).cpu().detach().numpy()\n","\n","### Feature affinities\n","W_feat = (feats @ feats.T)\n","W_feat = (W_feat * (W_feat > 0))\n","W_feat = W_feat / W_feat.max()\n","\n","### Color affinities\n","# If we are fusing with color affinites, then load the image and compute\n","image_color_lambda = 5\n","if image_color_lambda > 0:\n","    W_lr = knn_affinity(image, n_neighbors=[20, 10, 5], distance_weights=[0.1, 1.0, 2.0])\n","    W_color = np.array(W_lr.todense().astype(np.float32))\n","else:\n","    # No color affinity\n","    W_color = 0\n","\n","# combine affinity matrices\n","W_comb = W_feat + (W_color * image_color_lambda)\n","\n","# compute laplacian of the affinity matrix\n","L, D = compute_laplacian(W_comb, lap=laplacian, s=1.0)\n","\n","# compute eigenvectors of the laplacian and extract second smallest eigenvector\n","eigenvecs = compute_eigs(L, D, K=2)\n","second_eigenvec = eigenvecs[1].numpy()\n","\n","# get salient area\n","bipartition = get_salient_areas(second_eigenvec)\n","\n","# check if we should reverse the partition\n","dims, scales = [H_patch, H_patch], [patch_size, patch_size]\n","seed = np.argmax(np.abs(second_eigenvec))\n","\n","reverse = False\n","nc = check_num_fg_corners(bipartition, dims)\n","if nc == 4:\n","  reverse = True\n","\n","if reverse:\n","  # reverse bipartition, eigenvector and get new seed\n","  second_eigenvec = second_eigenvec * -1\n","  bipartition = np.logical_not(bipartition)\n","  seed = np.argmax(second_eigenvec)\n","\n","# get pixels corresponding to the seed\n","bipartition = bipartition.reshape(dims).astype(float)\n","_, _, _, cc = detect_box(bipartition, seed, dims, scales=scales, initial_im_size=dims)\n","pseudo_mask = np.zeros(dims)\n","pseudo_mask[cc[0],cc[1]] = 1\n","pseudo_mask = torch.from_numpy(pseudo_mask)\n","\n","# upsample pseudo mask to initial image shape\n","bipartition = F.interpolate(pseudo_mask.unsqueeze(0).unsqueeze(0), size=dims, mode='nearest').squeeze()\n","bipartition = bipartition.cpu().numpy()\n","bipartition[bipartition <= 0] = 0\n","\n","# post-process pesudo-masks with CRF\n","pseudo_mask = densecrf(np.array(image), bipartition)\n","pseudo_mask = ndimage.binary_fill_holes(pseudo_mask>=0.5)\n","\n","# construct binary pseudo-masks\n","pseudo_mask[pseudo_mask < 0] = 0\n","pseudo_mask = Image.fromarray(np.uint8(pseudo_mask*255))\n","pseudo_mask = np.asarray(pseudo_mask.resize((fixed_size, fixed_size)))\n","\n","pseudo_mask = pseudo_mask.astype(np.uint8)\n","upper = np.max(pseudo_mask)\n","lower = np.min(pseudo_mask)\n","thresh = upper / 2.0\n","pseudo_mask[pseudo_mask > thresh] = upper\n","pseudo_mask[pseudo_mask <= thresh] = lower"]},{"cell_type":"markdown","metadata":{"id":"wbT4ZazJoIHW"},"source":["## Plot the salient object mask"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1731773252519,"user":{"displayName":"Tibet Akinci","userId":"15227757771099747897"},"user_tz":-60},"id":"PacR-zvnebxm","outputId":"ab812c81-ec35-4807-dc6f-40ae32c5dc15"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["f, axarr = plt.subplots(2, 1)\n","axarr[0].imshow(image_rgb)\n","axarr[0].axis('off')\n","axarr[1].imshow(pseudo_mask)\n","axarr[1].axis('off')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1pnv3GoLXrOy_-_UvQRy0iNzjuV1j9cp1","timestamp":1731767855418}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
